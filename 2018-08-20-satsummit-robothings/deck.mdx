import { Head } from 'mdx-deck'

export { swiss as theme } from 'mdx-deck/themes'

<Head>
  <title>RoboThings</title>
</Head>

```notes
- TODO: Create a slide deck
- TODO: Flush out the hard negative mining bit
- TODO: Bundle prepared data
- TODO: Find a bunch of flash drives
- TODO: CFN template for SageMaker
    - [Get RoboSat off of morecs and onto SageMaker](https://github.com/mapbox/robosat-internal/issues/192)
    - [RoboSat python3 environment on SageMaker](https://github.com/mapbox/sagemaker/issues/2)
- TODO: Try to integrate RoboSat with LabelMaker
```

![](https://gist.githubusercontent.com/daniel-j-h/20f4c668f683336764041a9a5eab7b1e/raw/124ee3d7d7628c1d60d3609d0b6515995ed2781d/pipeline.gif)

 üôè  [Daniel](https://github.com/daniel-j-h) and [Bhargav](https://github.com/bkowshik/).

```notes
The prediction core is a segmentation model ‚Äî a fully convolutional neural net which we train on pairs of images and masks. 

The aerial imagery we download from our Mapbox Maps API in all its beauty. 

The masks we extract from OpenStreetMap geometries and rasterize them into image tiles. 

These geometries might sometimes be coarsely mapped but automatically extracting masks allows us to quickly bootstrap a dataset for training.

Features can be anything visually distinguishable - buildings, parking lots, roads, etc.

You don't need to know or understand the network architecture if you don't want to.

You need to bring imagery and masks.
```

---

# Why did we think this was interesting?

- Streamline [OSM changeset](https://wiki.openstreetmap.org/wiki/Changeset) validation.
- Identify unmapped features in OSM.
- Help to quantify map completion for particular features.
- Enhancement of our existing processing piplines.

```notes
RoboSat can "look" at every edit in OpenStreetMap in real-time to flag suspicious changesets. At the same time it can help to let good looking changesets go through without manual verification.

RoboSat can tell you how complete the map is in a specific area for a specific feature. For example: "Buildings in Seattle are 90% mapped". And then it can show you unmapped buildings and polygon recommendations for them.

RoboSat can be integrated into imagery platforms like OpenAerialMap or toolchains like OpenDroneMap to generate a better understanding of the area minutes after flying your drone.
```

---

# Network architecture

- [U-Net](https://arxiv.org/abs/1505.04597)
- [ResNet](https://arxiv.org/abs/1512.03385) 50 encoder, symmetrical decoder.

Previously:
- a [Pyramid Scene Parsing Network](https://arxiv.org/abs/1612.01105)

Future:
- [Feature Pyramid Network](https://arxiv.org/abs/1612.03144) 
- [RetinaNet](https://arxiv.org/abs/1708.02002), and it's open [Pull Request](https://github.com/mapbox/robosat/pull/75)

```notes
Design goals:

1. Stay simple
2. Do a good job

We follow the 80/20 rule where 80% of the effects come from 20% of the causes: we strive for simplicity and maintainability over pixel-perfect results. If you can improve the model's accuracy by two percent points but have to add thousands of lines of code we most likely won't accept your changeset.

The encoder turns the image into a fixed size feature vector.
The decoder turns those features back into full-resolution masks.

Skip connections link encoder blocks deeper into the decoder, so that more of the high resolution information can be passed through. Without these connections, it would be much harder to decode the encoder's last output on it's own.


PSPNet

Good for multiclass detections, and it's "context block" allows for the network to understand that if it identifies roads, it's then more likely to also detect cars, and less likely to detect water. Training and prediction were both very slow.

FPNs are similar to u-nets. Channel addition rather than concatenation. FPN is commonly used as a base for other state-of-the-art networks, like RetinaNet.
```

---

# Data sources

RoboSat is data agnostic so long as your imagery can be transformed into a [slippy map format](https://wiki.openstreetmap.org/wiki/Slippy_map_tilenames).

An API will make your life simpler though!

- [mapbox.satellite](https://www.mapbox.com/maps/satellite/)
- [OpenAerialMap](https://openaerialmap.org/)
- [DigitalGlobe](https://platform.digitalglobe.com/maps-api/)
- [Planet](https://developers.planet.com/docs/api/tile-services/)
- üíÅ‚Äç‚ôÇÔ∏è



```notes
Your source is going to impact the features you can predict. Higher zoom, smaller features.
```

---

# Today's plan

1. ~~Prepare some data~~
2. Train a model
3. Evaluate our predicted buildings

```notes
As mentioned, RoboSat provides a parallel pathway to LabelMaker.

Didn't bring your graphics card? That's fine - we have intermediate datasets you can work with while you're here.
```

---

# Setup & Requirements

---

## Option 1: From source

If you're running some form of *nix and want to install from source, see the instructions in the [README](https://github.com/mapbox/robosat#installation).

```notes
We don't have enough time to cover installation today.
```

---

## Option 2: Docker

#### With a GPU

```sh
# configure nvidia-docker
# See https://github.com/NVIDIA/nvidia-docker#quickstart

$ docker pull mapbox/robosat:latest-gpu
$ docker run --runtime=nvidia -it mapbox/robosat:latest-gpu /bin/bash

# Check that robosat is available
$ ./rs --help
```

#### Without a GPU

```sh
$ docker pull mapbox/robosat:latest-cpu
$ docker run -it mapbox/robosat:latest-cpu /bin/bash

# Check that robosat is available
$ ./rs --help
```

Given our time, this is what I recommend doing today.

---

## Option X: Something else

We've used Nvidia GTX 1080 TIs and AWS' p2 & p3 instances.

If you want to train efficiently, you'll want a GPU.

Prediction can be run on either GPU or CPU.

---

## Create a training dataset

We need **images** and **masks** for those features we want to predict.

![](https://cldup.com/01d2P1qI7b-3000x3000.png)

```notes
rs extract
Extract the category of GeoJSON features we're interesting in predicting from OpenStreetMap

rs cover
Determine which tiles overlap those features

rs download
Download tiles from a map endpoint

rs rasterize
Rasterize the GeoJSON features which you're interested in predicting into masks which we'll train our model with.
```

---

## Train a model 

Generate the segmentation models we'll predict with.

![](https://cldup.com/KfV3Z6Ihz6-3000x3000.png)

```notes
rs train
Using the `(image, mask)` pairs, generate checkpoints containing our weights

rs predict
Predict class probabilities for each image in our slippy map directory using those weights

rs masks
Generate segmentation masks for each class probability
```

---

## Post processing

Clean up and simplify our predictions.

![](https://cldup.com/3dsvnGSVEg-3000x3000.png)

```notes
rs features
Extract simplified GeoJSON features from segmentation masks.

rs merge
Merge adjacent features into single features

rs dedupe
Deduplicated predicted features against existing OSM features
```

---

# Let's start!

We're going to follow along with Daniel's OSM Diary entry, [RoboSat loves Tanzania](https://www.openstreetmap.org/user/daniel-j-h/diary/44321).

And we're going to skip all the way ahead to the training section.

Here's a [preview](https://s3.amazonaws.com/robosat-public/28f28ffa-6b4e-46ee-91dc-a6404790a491/probabilities2/index.html#18.5/-6.1933296/39.2698587
) of what we're going to try and build.

```
# data on S3
s3://robosat-public/28f28ffa-6b4e-46ee-91dc-a6404790a491/
```

---

# RoboGlasses

Super-resolution for aerial or satellite imagery.

![](https://user-images.githubusercontent.com/527241/45626591-30b49c80-ba90-11e8-8f7d-9b801f806329.gif) ![](https://user-images.githubusercontent.com/527241/45626592-30b49c80-ba90-11e8-9288-59b5a4314e63.gif) ![](https://user-images.githubusercontent.com/527241/45626593-30b49c80-ba90-11e8-9dd7-b93627ace7bd.gif)

Can we predict Sentinel at z16?

```notes
Idea:

Using RoboGlasses:
- Train on aerial imagery to learn how to go from z14 to z15, or z15 to z16.
- Predict on Sentinel at z14.

Using Robosat:
- Train on the predicted Sentinel
- Predict on Sentinel at z16!!!

In some cases, roads in Sentinel are only 1 or 2 pixels wide. If we can extend that to z16, maybe we can get aerial-esque results in regions where we don't have aerial coverage.
```
